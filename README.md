# BiasDetector - D√©tection Intelligente de Biais avec IA Locale

BiasDetector est un outil d'analyse de documents qui utilise l'Intelligence Artificielle **100% locale** pour d√©tecter automatiquement diff√©rents types de biais dans les textes.

**Aucune cl√© API requise** - Tout fonctionne sur votre machine.

---

## Table des Mati√®res

- [Fonctionnalit√©s](#fonctionnalit√©s)
- [Pr√©requis](#pr√©requis)
- [Installation sur macOS](#installation-sur-macos)
- [Lancement du Projet](#lancement-du-projet)
- [Acc√®s √† l'Application](#acc√®s-√†-lapplication)
- [Configuration](#configuration)
- [Architecture](#architecture)
- [D√©pannage](#d√©pannage)

---

## Fonctionnalit√©s

- **Analyse de Documents** : Upload de PDF, TXT, DOCX avec d√©tection de 6 types de biais
- **RAG (Retrieval Augmented Generation)** : Analyse contextuelle bas√©e sur l'historique
- **Chat IA** : Posez des questions sur les patterns de biais d√©tect√©s
- **Recherche S√©mantique** : Trouvez des documents similaires par le sens
- **Statistiques** : Visualisez les tendances de biais dans vos documents
- **100% Priv√©** : Vos documents ne quittent jamais votre machine

### Types de Biais D√©tect√©s

| Biais | Description |
|-------|-------------|
| Genre | St√©r√©otypes bas√©s sur le genre |
| Politique | Orientation politique partisane |
| Culturel | Ethnocentrisme, suppositions culturelles |
| Confirmation | Recherche d'infos confirmant ses croyances |
| S√©lection | Cherry-picking de donn√©es |
| Ancrage | D√©pendance excessive √† l'info initiale |

---

## Pr√©requis

Avant de commencer, installez les logiciels suivants sur votre Mac :

| Logiciel | Version | Installation |
|----------|---------|--------------|
| **Python** | 3.11+ | `brew install python` ou [python.org](https://python.org) |
| **Node.js** | 18+ | `brew install node` ou [nodejs.org](https://nodejs.org) |
| **Docker Desktop** | Latest | [T√©l√©charger Docker Desktop](https://www.docker.com/products/docker-desktop/) |
| **Ollama** | Latest | [T√©l√©charger Ollama](https://ollama.ai) |

### V√©rifier les installations

```bash
python3 --version   # Doit afficher Python 3.11+
node --version      # Doit afficher v18+
docker --version    # Doit afficher Docker version 20+
ollama --version    # Doit afficher ollama version 0.x
```

---

## Installation sur macOS

### √âtape 1 : T√©l√©charger le projet

```bash
# Cloner le projet (ou t√©l√©charger le ZIP)
git clone <url-du-projet>
cd BiasDetector
```

### √âtape 2 : Installer et configurer Ollama

```bash
# T√©l√©charger les mod√®les requis
ollama pull llama3.2:3b
ollama pull nomic-embed-text
```

> **Note** : Le t√©l√©chargement peut prendre quelques minutes selon votre connexion.

### √âtape 3 : Configurer le Backend

```bash
# Aller dans le dossier backend
cd backend

# Cr√©er l'environnement virtuel Python
python3 -m venv venv

# Activer l'environnement virtuel
source venv/bin/activate

# Installer les d√©pendances
pip install --upgrade pip
pip install -r requirements.txt
```

### √âtape 4 : Configurer le Frontend

```bash
# Aller dans le dossier frontend
cd ../frontend

# Installer les d√©pendances Node.js
npm install
```

---

## Lancement du Projet

### Ordre de lancement important

Lancez les services dans cet ordre :

### 1. Lancer Docker Desktop

Ouvrez l'application **Docker Desktop** et attendez que l'ic√¥ne devienne verte (pr√™t).

### 2. Lancer MongoDB

```bash
# Dans un nouveau terminal
docker run -d -p 27017:27017 --name biasdetector-mongodb mongo:latest
```

> Si le conteneur existe d√©j√† : `docker start biasdetector-mongodb`

### 3. Lancer Ollama

```bash
# Dans un nouveau terminal
ollama serve
```

> Laissez ce terminal ouvert. Ollama doit tourner en permanence.

### 4. Lancer le Backend

```bash
# Dans un nouveau terminal
cd BiasDetector/backend
source venv/bin/activate
python -m uvicorn main:app --host 0.0.0.0 --port 8000
```

Attendez de voir :
```
INFO:     Uvicorn running on http://0.0.0.0:8000
```

### 5. Lancer le Frontend

```bash
# Dans un nouveau terminal
cd BiasDetector/frontend
npm run dev
```

Attendez de voir :
```
‚úì Ready in X.Xs
- Local: http://localhost:3000
```

> Si le port 3000 est occup√©, Next.js utilisera automatiquement le port 3001.

---

## Acc√®s √† l'Application

Une fois tous les services lanc√©s :

| Service | URL |
|---------|-----|
| **Application Web** | http://localhost:3000 (ou 3001) |
| **API Backend** | http://localhost:8000 |
| **Documentation API (Swagger)** | http://localhost:8000/docs |
| **Documentation API (ReDoc)** | http://localhost:8000/redoc |

---

## Configuration

### Variables d'environnement Backend

Le fichier `backend/.env` contient la configuration :

```env
# Ollama (IA locale)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b
OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# MongoDB
MONGODB_URL=mongodb://localhost:27017
MONGODB_DATABASE=biasdetector

# RAG
RAG_ENABLED=True
RAG_MAX_CONTEXT_CHUNKS=5
RAG_RELEVANCE_THRESHOLD=0.7

# Fichiers
MAX_UPLOAD_SIZE=10485760
UPLOAD_DIR=./uploads
```

### Changer de mod√®le Ollama

Pour utiliser un mod√®le diff√©rent :

```bash
# T√©l√©charger un autre mod√®le
ollama pull mistral

# Modifier backend/.env
OLLAMA_MODEL=mistral
```

Mod√®les recommand√©s :
- `llama3.2:3b` - √âquilibr√© (2GB) - **Recommand√©**
- `mistral` - Tr√®s bon (4GB)
- `llama3.2` - Plus puissant (4GB)

---

## Architecture

```
BiasDetector/
‚îú‚îÄ‚îÄ backend/                 # API FastAPI (Python)
‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/endpoints/   # Routes API
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/        # Logique m√©tier
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ollama_service.py    # Service IA
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chroma_service.py    # Service LanceDB
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rag_service.py       # Service RAG
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ database_service.py  # Service MongoDB
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ models/          # Sch√©mas Pydantic
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # Point d'entr√©e
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt     # D√©pendances Python
‚îÇ   ‚îî‚îÄ‚îÄ .env                 # Configuration
‚îÇ
‚îú‚îÄ‚îÄ frontend/                # Interface Next.js (React)
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ app/             # Pages
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/      # Composants React
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hooks/           # Hooks personnalis√©s
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ lib/             # Utilitaires
‚îÇ   ‚îî‚îÄ‚îÄ package.json         # D√©pendances Node.js
‚îÇ
‚îî‚îÄ‚îÄ README.md                # Ce fichier
```

### Flux de donn√©es

```
[Document Upload] ‚Üí [Extraction Texte] ‚Üí [Chunking]
                                              ‚Üì
[LanceDB] ‚Üê [Embeddings] ‚Üê [Ollama nomic-embed-text]
    ‚Üì
[Recherche RAG] ‚Üí [Contexte Similaire]
                        ‚Üì
              [Ollama llama3.2] ‚Üí [Analyse de Biais]
                                        ‚Üì
                              [MongoDB] ‚Üí [R√©sultats]
```

---

## D√©pannage

### Docker ne d√©marre pas

**Probl√®me** : `Cannot connect to Docker daemon`

**Solution** :
1. Ouvrez Docker Desktop manuellement
2. Attendez que l'ic√¥ne devienne verte
3. R√©essayez la commande

### Ollama ne r√©pond pas

**Probl√®me** : `Connection refused on port 11434`

**Solution** :
```bash
# V√©rifier si Ollama tourne
curl http://localhost:11434/api/tags

# Si erreur, lancer Ollama
ollama serve
```

### Mod√®les Ollama manquants

**Probl√®me** : `Model not found`

**Solution** :
```bash
ollama pull llama3.2:3b
ollama pull nomic-embed-text
```

### Port d√©j√† utilis√©

**Probl√®me** : `Port 3000 is in use`

**Solution** : Next.js utilisera automatiquement le port 3001. Acc√©dez √† http://localhost:3001

Pour lib√©rer le port manuellement :
```bash
# Trouver le processus
lsof -i :3000

# Tuer le processus (remplacer PID)
kill -9 <PID>
```

### MongoDB ne d√©marre pas

**Probl√®me** : `Connection refused on port 27017`

**Solution** :
```bash
# V√©rifier si le conteneur existe
docker ps -a | grep mongo

# Si existe mais arr√™t√©
docker start biasdetector-mongodb

# Si n'existe pas, cr√©er
docker run -d -p 27017:27017 --name biasdetector-mongodb mongo:latest
```

### Erreurs Python (ModuleNotFoundError)

**Solution** :
```bash
cd backend
source venv/bin/activate
pip install -r requirements.txt --upgrade
```

### Erreurs npm

**Solution** :
```bash
cd frontend
rm -rf node_modules package-lock.json
npm install
```

### Espace disque insuffisant

**Probl√®me** : `No space left on device`

**Solution** :
```bash
# Nettoyer le cache pip
pip cache purge

# Nettoyer Docker
docker system prune -a

# D√©placer le projet sur un disque avec plus d'espace
```

---

## Script de Lancement Rapide (macOS)

Cr√©ez un fichier `start.sh` √† la racine du projet :

```bash
#!/bin/bash

echo "=== BiasDetector - D√©marrage ==="

# V√©rifier Docker
if ! docker info > /dev/null 2>&1; then
    echo "‚ùå Docker n'est pas d√©marr√©. Lancez Docker Desktop."
    exit 1
fi
echo "‚úÖ Docker OK"

# Lancer MongoDB si pas d√©j√† lanc√©
if ! docker ps | grep -q biasdetector-mongodb; then
    echo "üöÄ D√©marrage MongoDB..."
    docker run -d -p 27017:27017 --name biasdetector-mongodb mongo:latest 2>/dev/null || docker start biasdetector-mongodb
fi
echo "‚úÖ MongoDB OK"

# V√©rifier Ollama
if ! curl -s http://localhost:11434/api/tags > /dev/null; then
    echo "‚ö†Ô∏è  Ollama n'est pas d√©marr√©. Lancez 'ollama serve' dans un autre terminal."
    exit 1
fi
echo "‚úÖ Ollama OK"

# Lancer Backend
echo "üöÄ D√©marrage Backend..."
cd backend
source venv/bin/activate
python -m uvicorn main:app --host 0.0.0.0 --port 8000 &
BACKEND_PID=$!
cd ..

sleep 5

# Lancer Frontend
echo "üöÄ D√©marrage Frontend..."
cd frontend
npm run dev &
FRONTEND_PID=$!
cd ..

echo ""
echo "=== BiasDetector est pr√™t ! ==="
echo "Frontend: http://localhost:3000"
echo "Backend:  http://localhost:8000"
echo ""
echo "Appuyez sur Ctrl+C pour arr√™ter tous les services"

# Attendre et nettoyer
trap "kill $BACKEND_PID $FRONTEND_PID 2>/dev/null" EXIT
wait
```

Rendez-le ex√©cutable et lancez-le :
```bash
chmod +x start.sh
./start.sh
```

---

## Utilisation

1. **Ouvrez** http://localhost:3000 dans votre navigateur
2. **Uploadez** un document (PDF, TXT, ou DOCX)
3. **Cliquez** sur "Analyze" pour lancer l'analyse
4. **Consultez** les biais d√©tect√©s avec les citations et suggestions
5. **Utilisez** le chat RAG pour poser des questions sur vos analyses

---

## Licence

MIT License - Libre d'utilisation

---

Projet acad√©mique - Sp√©cialisation IA&BD/CCV
